{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pip install opencv-python"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T10:48:48.026279Z","iopub.status.busy":"2024-02-28T10:48:48.025369Z","iopub.status.idle":"2024-02-28T10:48:48.030785Z","shell.execute_reply":"2024-02-28T10:48:48.029849Z","shell.execute_reply.started":"2024-02-28T10:48:48.026247Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.15.0\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import *\n","from tensorflow.keras.applications import mobilenet_v2\n","from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n","\n","print(tf.__version__)"]},{"cell_type":"markdown","metadata":{},"source":["# Problem Statement"]},{"cell_type":"markdown","metadata":{},"source":["#### This project intends to provide an alternative to the fingerprint scanners at our boarding school using facial recognition. The project only intends to cover the backend, and there will be no physical prototype. Hardware used for demonstration will be web camera from computer."]},{"cell_type":"markdown","metadata":{},"source":["# Data Collection"]},{"cell_type":"markdown","metadata":{},"source":["The way we intend to obtain our training data is to get a few images of the students and perform one-shot learning with them, such that we do not have to waste the students' time with gathering large quantities of pictures for each of them."]},{"cell_type":"markdown","metadata":{},"source":["Firstly, we will be using opencv to make use of the webcam as the input device, since we are not building a physical prototype. We also use a haar cascade classifier in order to detect whether there are faces in the webcame image."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use the first video capturing device on your device\n","video=cv2.VideoCapture(0)\n","\n","# For this, copy the absolute path and replace it with your own\n","facedetect=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n","\n","# Check if cascade classifier loaded (whether path to cascade classifier is valid)\n","if facedetect.empty():\n","    print(\"Error: Cascade Classifier not loaded\")\n","else:\n","    print(\"Cascade Classifier loaded successfully\")"]},{"cell_type":"markdown","metadata":{},"source":["We then run this chunk where the user will be asked to input their name (not case-sensitive) and the webcam will start capturing images of the person if there is a face in the webcam. The images are then cropped such that only the face is visible in the image. 20 pictures are taken of the student and saved to a folder."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["count=0\n","nameID=str(input(\"Enter Your Name: \")).lower()\n","\n","path='images/'+nameID\n","isExist = os.path.exists(path)\n","\n","# If name is already taken, ask for another one\n","if isExist:\n","\tprint(\"Name Already Taken\")\n","\tnameID=str(input(\"Enter Your Name Again: \"))\n","else:\n","\tos.makedirs(path)\n","\n","while True:\n","\t# Start reading input from webcam, shows webcam video capture on device\n","\tret,frame=video.read()\n","\tfaces=facedetect.detectMultiScale(frame,1.3, 5)\n","\tfor x,y,w,h in faces:\n","\t\tcount=count+1\n","\t\tname='./images/'+nameID+'/'+ str(count) + '.jpg'\n","\t\tprint(\"Creating Images.........\" +name)\n","\t\tcv2.imwrite(name, frame[y:y+h,x:x+w])\n","\t\tcv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,0), 3)\n","\tcv2.imshow(\"WindowFrame\", frame)\n","\tcv2.waitKey(1)\n","\tif count>10:\n","\t\tbreak\n","\n","# Stops reading input from webcam, removes webcam video capture on device\n","video.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["Since our aim is to perform one shot learning, we do not need to perform any preprocessing on our data"]},{"cell_type":"markdown","metadata":{},"source":["# Deep Learning Model: Naive CNN"]},{"cell_type":"markdown","metadata":{},"source":["Now, we will start piecing together our CNN models. Firstly, we will create our callback function that we will be inputting into compile function such that we can prevent the model from training after reaching a satisfactory state, causing it to overfit."]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T10:48:55.493059Z","iopub.status.busy":"2024-02-28T10:48:55.492811Z","iopub.status.idle":"2024-02-28T10:48:55.498194Z","shell.execute_reply":"2024-02-28T10:48:55.497253Z","shell.execute_reply.started":"2024-02-28T10:48:55.493038Z"},"trusted":true},"outputs":[],"source":["class myCallback(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs={}):\n","    if(logs.get('val_loss') < 0.25):\n","      print(\"Cancelling training.\")\n","      self.model.stop_training = True\n","\n","callbacks = myCallback()"]},{"cell_type":"markdown","metadata":{},"source":["Then we assemble and train our naive CNN model."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T10:48:55.499570Z","iopub.status.busy":"2024-02-28T10:48:55.499296Z","iopub.status.idle":"2024-02-28T10:48:55.518501Z","shell.execute_reply":"2024-02-28T10:48:55.517635Z","shell.execute_reply.started":"2024-02-28T10:48:55.499549Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n"]}],"source":["naivemodel = tf.keras.models.Sequential([\n","    data_preprocessing,\n","    data_augmentation,\n","    tf.keras.layers.Conv2D(16, 3, padding = 'same', activation = 'relu'),\n","    tf.keras.layers.MaxPooling2D(),\n","    tf.keras.layers.Conv2D(32, 3, padding = 'same', activation = 'relu'),\n","    tf.keras.layers.MaxPooling2D(),\n","    tf.keras.layers.Conv2D(64, 3, padding = 'same', activation = 'relu'),\n","    tf.keras.layers.MaxPooling2D(),\n","    tf.keras.layers.Dropout(0.2),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n","    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n","    tf.keras.layers.Dense(10)\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["naivemodel.compile(optimizer=tf.optimizers.Adam(),\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T10:48:55.520454Z","iopub.status.busy":"2024-02-28T10:48:55.519657Z","iopub.status.idle":"2024-02-28T10:56:52.310486Z","shell.execute_reply":"2024-02-28T10:56:52.309591Z","shell.execute_reply.started":"2024-02-28T10:48:55.520395Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["2024-02-28 10:49:00.688228: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/sequential_5_1/dropout_1_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 446ms/step - accuracy: 0.1135 - loss: 3.7554 - val_accuracy: 0.1510 - val_loss: 2.2782\n","Epoch 2/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 438ms/step - accuracy: 0.1423 - loss: 2.2735 - val_accuracy: 0.2525 - val_loss: 2.1211\n","Epoch 3/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 445ms/step - accuracy: 0.2506 - loss: 2.1051 - val_accuracy: 0.2810 - val_loss: 2.0120\n","Epoch 4/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 444ms/step - accuracy: 0.2865 - loss: 2.0051 - val_accuracy: 0.2985 - val_loss: 1.9774\n","Epoch 5/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 445ms/step - accuracy: 0.3000 - loss: 1.9603 - val_accuracy: 0.2995 - val_loss: 1.9587\n","Epoch 6/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 444ms/step - accuracy: 0.3070 - loss: 1.9494 - val_accuracy: 0.3235 - val_loss: 1.9156\n","Epoch 7/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 446ms/step - accuracy: 0.3317 - loss: 1.8886 - val_accuracy: 0.3250 - val_loss: 1.9326\n","Epoch 8/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 443ms/step - accuracy: 0.3362 - loss: 1.8808 - val_accuracy: 0.3505 - val_loss: 1.8747\n","Epoch 9/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 456ms/step - accuracy: 0.3524 - loss: 1.8376 - val_accuracy: 0.3115 - val_loss: 1.9512\n","Epoch 10/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 443ms/step - accuracy: 0.3411 - loss: 1.8562 - val_accuracy: 0.3470 - val_loss: 1.8527\n"]}],"source":["naivemodel.compile(optimizer=tf.optimizers.Adam(),\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","naivehistory = naivemodel.fit(train, validation_data=val, epochs=10, callbacks=[callbacks])"]},{"cell_type":"markdown","metadata":{},"source":["# Deep Learning Model: Transfer Learning CNN "]},{"cell_type":"markdown","metadata":{},"source":["Next, we will be doing transfer learning with the InceptionV3 model from TensorFlow."]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T10:56:52.314497Z","iopub.status.busy":"2024-02-28T10:56:52.314166Z","iopub.status.idle":"2024-02-28T10:56:54.429974Z","shell.execute_reply":"2024-02-28T10:56:54.428948Z","shell.execute_reply.started":"2024-02-28T10:56:52.314471Z"},"trusted":true},"outputs":[],"source":["inceptionthemovie = tf.keras.applications.inception_v3.InceptionV3(\n","    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n","    include_top=False\n",")\n","inceptionthemovie.trainable = False"]},{"cell_type":"markdown","metadata":{},"source":["Finally, we piece together our second model and start training it."]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T10:56:54.431431Z","iopub.status.busy":"2024-02-28T10:56:54.431124Z","iopub.status.idle":"2024-02-28T10:56:54.454683Z","shell.execute_reply":"2024-02-28T10:56:54.453749Z","shell.execute_reply.started":"2024-02-28T10:56:54.431407Z"},"trusted":true},"outputs":[],"source":["input = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n","temp = data_augmentation(input)\n","temp = tf.keras.applications.inception_v3.preprocess_input(temp)\n","temp = inceptionthemovie(temp, training=False)\n","temp = tf.keras.layers.GlobalAveragePooling2D()(temp)\n","\n","V3model = tf.keras.Model(input, tf.keras.layers.Dense(10)(temp))"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T10:56:54.456109Z","iopub.status.busy":"2024-02-28T10:56:54.455803Z","iopub.status.idle":"2024-02-28T11:07:18.862820Z","shell.execute_reply":"2024-02-28T11:07:18.861872Z","shell.execute_reply.started":"2024-02-28T10:56:54.456086Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 589ms/step - accuracy: 0.5062 - loss: 1.6512 - val_accuracy: 0.8340 - val_loss: 0.7165\n","Epoch 2/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.7858 - loss: 0.7689 - val_accuracy: 0.8465 - val_loss: 0.5813\n","Epoch 3/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 563ms/step - accuracy: 0.7973 - loss: 0.6751 - val_accuracy: 0.8560 - val_loss: 0.5219\n","Epoch 4/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 566ms/step - accuracy: 0.8072 - loss: 0.6295 - val_accuracy: 0.8610 - val_loss: 0.4958\n","Epoch 5/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8123 - loss: 0.6098 - val_accuracy: 0.8655 - val_loss: 0.4819\n","Epoch 6/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8142 - loss: 0.5870 - val_accuracy: 0.8675 - val_loss: 0.4719\n","Epoch 7/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8143 - loss: 0.5774 - val_accuracy: 0.8635 - val_loss: 0.4630\n","Epoch 8/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 567ms/step - accuracy: 0.8251 - loss: 0.5557 - val_accuracy: 0.8590 - val_loss: 0.4659\n","Epoch 9/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 574ms/step - accuracy: 0.8213 - loss: 0.5612 - val_accuracy: 0.8650 - val_loss: 0.4623\n","Epoch 10/10\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 568ms/step - accuracy: 0.8288 - loss: 0.5301 - val_accuracy: 0.8710 - val_loss: 0.4680\n"]}],"source":["V3model.compile(optimizer=tf.optimizers.Adam(),\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","V3history = V3model.fit(train, validation_data=val, epochs=10, callbacks=[callbacks])"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation of Models"]},{"cell_type":"markdown","metadata":{},"source":["# Hyperparameter Tuning"]},{"cell_type":"markdown","metadata":{},"source":["Now, we will tune our model. First, we remake the model that we chose, that being the V3model so that we can tune it."]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T11:07:47.287079Z","iopub.status.busy":"2024-02-28T11:07:47.286725Z","iopub.status.idle":"2024-02-28T11:07:49.180786Z","shell.execute_reply":"2024-02-28T11:07:49.180020Z","shell.execute_reply.started":"2024-02-28T11:07:47.287046Z"},"trusted":true},"outputs":[],"source":["inceptionthemovie = tf.keras.applications.inception_v3.InceptionV3(\n","    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n","    include_top=False\n",")\n","inceptionthemovie.trainable = False\n","input = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n","temp = data_augmentation(input)\n","temp = tf.keras.applications.inception_v3.preprocess_input(temp)\n","temp = inceptionthemovie(temp, training=False)\n","temp = tf.keras.layers.GlobalAveragePooling2D()(temp)\n","\n","finalmodel = tf.keras.Model(input, tf.keras.layers.Dense(10)(temp))"]},{"cell_type":"markdown","metadata":{},"source":["Then we will use the keras tuner in order to find the best hyperparameters for our model. In this case, only the learning rate is applicable."]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T11:07:49.182100Z","iopub.status.busy":"2024-02-28T11:07:49.181834Z","iopub.status.idle":"2024-02-28T11:07:49.222727Z","shell.execute_reply":"2024-02-28T11:07:49.221753Z","shell.execute_reply.started":"2024-02-28T11:07:49.182077Z"},"trusted":true},"outputs":[],"source":["import keras_tuner as kt\n","def model_builder(hp):\n","    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n","    # hp_dropout = hp.Float('dropout', min_value=0.1, max_value=0.8, step=0.1) #unused\n","    # hp_dense_units = hp.Int('units', min_value=128, max_value=1024, step=64) #unused\n","\n","    model = finalmodel\n","\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n","                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                metrics=['accuracy'])\n","    \n","    return model\n","\n","\n","tuner = kt.Hyperband(model_builder,\n","                     objective='val_accuracy',\n","                     max_epochs=30,\n","                     factor=3,\n","                     directory='dir')"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T11:07:49.224121Z","iopub.status.busy":"2024-02-28T11:07:49.223800Z","iopub.status.idle":"2024-02-28T11:14:19.783101Z","shell.execute_reply":"2024-02-28T11:14:19.782128Z","shell.execute_reply.started":"2024-02-28T11:07:49.224096Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 3 Complete [00h 02m 09s]\n","val_accuracy: 0.8529999852180481\n","\n","Best val_accuracy So Far: 0.8585000038146973\n","Total elapsed time: 00h 06m 31s\n"]}],"source":["tuner.search(train, epochs=30, validation_data=val)\n","best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"]},{"cell_type":"markdown","metadata":{},"source":["Finally, we attempt to fit the model to find the epoch with the best accuracy before we do a final evaluation of our model."]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T11:14:19.784412Z","iopub.status.busy":"2024-02-28T11:14:19.784112Z","iopub.status.idle":"2024-02-28T11:44:05.394484Z","shell.execute_reply":"2024-02-28T11:44:05.393592Z","shell.execute_reply.started":"2024-02-28T11:14:19.784385Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 590ms/step - accuracy: 0.8345 - loss: 0.5455 - val_accuracy: 0.8660 - val_loss: 0.4584\n","Epoch 2/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 569ms/step - accuracy: 0.8300 - loss: 0.5451 - val_accuracy: 0.8685 - val_loss: 0.4545\n","Epoch 3/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 563ms/step - accuracy: 0.8315 - loss: 0.5270 - val_accuracy: 0.8695 - val_loss: 0.4530\n","Epoch 4/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8352 - loss: 0.5213 - val_accuracy: 0.8705 - val_loss: 0.4512\n","Epoch 5/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 560ms/step - accuracy: 0.8350 - loss: 0.5142 - val_accuracy: 0.8700 - val_loss: 0.4503\n","Epoch 6/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 562ms/step - accuracy: 0.8313 - loss: 0.5212 - val_accuracy: 0.8720 - val_loss: 0.4491\n","Epoch 7/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8308 - loss: 0.5099 - val_accuracy: 0.8695 - val_loss: 0.4486\n","Epoch 8/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8298 - loss: 0.5130 - val_accuracy: 0.8715 - val_loss: 0.4476\n","Epoch 9/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 566ms/step - accuracy: 0.8387 - loss: 0.5110 - val_accuracy: 0.8740 - val_loss: 0.4459\n","Epoch 10/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 563ms/step - accuracy: 0.8435 - loss: 0.4806 - val_accuracy: 0.8730 - val_loss: 0.4472\n","Epoch 11/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8298 - loss: 0.5171 - val_accuracy: 0.8735 - val_loss: 0.4442\n","Epoch 12/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 566ms/step - accuracy: 0.8385 - loss: 0.5086 - val_accuracy: 0.8720 - val_loss: 0.4449\n","Epoch 13/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8358 - loss: 0.5007 - val_accuracy: 0.8760 - val_loss: 0.4441\n","Epoch 14/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8354 - loss: 0.4921 - val_accuracy: 0.8740 - val_loss: 0.4443\n","Epoch 15/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 568ms/step - accuracy: 0.8407 - loss: 0.4903 - val_accuracy: 0.8715 - val_loss: 0.4445\n","Epoch 16/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 567ms/step - accuracy: 0.8431 - loss: 0.4803 - val_accuracy: 0.8740 - val_loss: 0.4422\n","Epoch 17/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 571ms/step - accuracy: 0.8404 - loss: 0.4927 - val_accuracy: 0.8720 - val_loss: 0.4424\n","Epoch 18/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8449 - loss: 0.4785 - val_accuracy: 0.8765 - val_loss: 0.4423\n","Epoch 19/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8427 - loss: 0.4846 - val_accuracy: 0.8745 - val_loss: 0.4415\n","Epoch 20/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 569ms/step - accuracy: 0.8442 - loss: 0.4748 - val_accuracy: 0.8740 - val_loss: 0.4411\n","Epoch 21/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8453 - loss: 0.4865 - val_accuracy: 0.8750 - val_loss: 0.4405\n","Epoch 22/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 568ms/step - accuracy: 0.8420 - loss: 0.4809 - val_accuracy: 0.8750 - val_loss: 0.4403\n","Epoch 23/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 562ms/step - accuracy: 0.8389 - loss: 0.4867 - val_accuracy: 0.8750 - val_loss: 0.4401\n","Epoch 24/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 561ms/step - accuracy: 0.8431 - loss: 0.4869 - val_accuracy: 0.8740 - val_loss: 0.4412\n","Epoch 25/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8443 - loss: 0.4739 - val_accuracy: 0.8710 - val_loss: 0.4405\n","Epoch 26/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8389 - loss: 0.4886 - val_accuracy: 0.8755 - val_loss: 0.4395\n","Epoch 27/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 566ms/step - accuracy: 0.8404 - loss: 0.4871 - val_accuracy: 0.8750 - val_loss: 0.4395\n","Epoch 28/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 569ms/step - accuracy: 0.8440 - loss: 0.4838 - val_accuracy: 0.8760 - val_loss: 0.4396\n","Epoch 29/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 566ms/step - accuracy: 0.8447 - loss: 0.4703 - val_accuracy: 0.8735 - val_loss: 0.4397\n","Epoch 30/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 560ms/step - accuracy: 0.8421 - loss: 0.4817 - val_accuracy: 0.8720 - val_loss: 0.4396\n"]}],"source":["tuningmodel = tuner.hypermodel.build(best_hps)\n","history = tuningmodel.fit(train, epochs=30, validation_data=val, callbacks=[callbacks])\n","\n","val_acc_per_epoch = history.history['val_accuracy']\n","best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T11:44:05.395933Z","iopub.status.busy":"2024-02-28T11:44:05.395648Z","iopub.status.idle":"2024-02-28T12:02:37.892276Z","shell.execute_reply":"2024-02-28T12:02:37.891307Z","shell.execute_reply.started":"2024-02-28T11:44:05.395907Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 595ms/step - accuracy: 0.8415 - loss: 0.4734 - val_accuracy: 0.8720 - val_loss: 0.4397\n","Epoch 2/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 567ms/step - accuracy: 0.8336 - loss: 0.4774 - val_accuracy: 0.8740 - val_loss: 0.4382\n","Epoch 3/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8455 - loss: 0.4807 - val_accuracy: 0.8760 - val_loss: 0.4379\n","Epoch 4/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 565ms/step - accuracy: 0.8456 - loss: 0.4779 - val_accuracy: 0.8740 - val_loss: 0.4389\n","Epoch 5/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 568ms/step - accuracy: 0.8446 - loss: 0.4756 - val_accuracy: 0.8725 - val_loss: 0.4386\n","Epoch 6/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 567ms/step - accuracy: 0.8478 - loss: 0.4612 - val_accuracy: 0.8750 - val_loss: 0.4384\n","Epoch 7/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 566ms/step - accuracy: 0.8451 - loss: 0.4666 - val_accuracy: 0.8730 - val_loss: 0.4387\n","Epoch 8/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 567ms/step - accuracy: 0.8467 - loss: 0.4738 - val_accuracy: 0.8720 - val_loss: 0.4390\n","Epoch 9/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8516 - loss: 0.4596 - val_accuracy: 0.8735 - val_loss: 0.4372\n","Epoch 10/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8416 - loss: 0.4805 - val_accuracy: 0.8755 - val_loss: 0.4377\n","Epoch 11/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 565ms/step - accuracy: 0.8431 - loss: 0.4714 - val_accuracy: 0.8760 - val_loss: 0.4370\n","Epoch 12/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8516 - loss: 0.4611 - val_accuracy: 0.8725 - val_loss: 0.4371\n","Epoch 13/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 563ms/step - accuracy: 0.8469 - loss: 0.4770 - val_accuracy: 0.8735 - val_loss: 0.4363\n","Epoch 14/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 568ms/step - accuracy: 0.8436 - loss: 0.4727 - val_accuracy: 0.8725 - val_loss: 0.4370\n","Epoch 15/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 566ms/step - accuracy: 0.8499 - loss: 0.4763 - val_accuracy: 0.8745 - val_loss: 0.4363\n","Epoch 16/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8497 - loss: 0.4613 - val_accuracy: 0.8735 - val_loss: 0.4365\n","Epoch 17/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 568ms/step - accuracy: 0.8501 - loss: 0.4600 - val_accuracy: 0.8740 - val_loss: 0.4375\n","Epoch 18/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 567ms/step - accuracy: 0.8512 - loss: 0.4645 - val_accuracy: 0.8755 - val_loss: 0.4362\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 433ms/step - accuracy: 0.8854 - loss: 0.4152\n"]},{"data":{"text/plain":["[0.4361916482448578, 0.8755000233650208]"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["finalmodel = tuner.hypermodel.build(best_hps)\n","\n","finalmodel.fit(train, epochs=best_epoch, validation_data=val)\n","finalmodel.evaluate(val)"]},{"cell_type":"markdown","metadata":{},"source":["# Final Model Evaluation\n","```py\n","before = [0.4680415689945221, 0.8709999918937683]\n","after = [0.4361916482448578, 0.8755000233650208]\n","```\n","From this, we can conclude that finetuning has increased the accuracy slightly by around 0.55% and decreased the loss by around 0.03, which means the hyperparameter tuning has worked and improved the model. Overall, the model is quite accurate and categorising pictures into the 10 different classes."]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3087285,"sourceId":5311975,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
