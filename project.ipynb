{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Installation of Libraries Required"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: opencv-python in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.9.0.80)\n","Requirement already satisfied: numpy in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.4)\n","Requirement already satisfied: tensorflow in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.15.0)\n","Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n","Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.10.0)\n","Requirement already satisfied: libclang>=13.0.0 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in c:\\users\\g0dph\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.25.2)\n","Requirement already satisfied: setuptools in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (65.5.0)\n","Requirement already satisfied: six>=1.12.0 in c:\\users\\g0dph\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.10.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.60.1)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.42.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.5.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.0)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.5)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\g0dph\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install opencv-python numpy tensorflow"]},{"cell_type":"markdown","metadata":{},"source":["## Importing Libraries & Defining the Image Capturing Device"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T10:48:48.026279Z","iopub.status.busy":"2024-02-28T10:48:48.025369Z","iopub.status.idle":"2024-02-28T10:48:48.030785Z","shell.execute_reply":"2024-02-28T10:48:48.029849Z","shell.execute_reply.started":"2024-02-28T10:48:48.026247Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\g0dph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n","2.15.0\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import *\n","from keras.layers import *\n","from tensorflow.keras.applications import mobilenet_v2\n","from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n","\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"text/plain":["False"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["cap=cv2.VideoCapture(0)\n","cap.set(3, 640)\n","cap.set(4, 480)"]},{"cell_type":"markdown","metadata":{},"source":["# Problem Statement"]},{"cell_type":"markdown","metadata":{},"source":["#### This project intends to provide an alternative to the fingerprint scanners at our boarding school using facial recognition. The project only intends to cover the backend, and there will be no physical prototype. Hardware used for demonstration will be web camera from computer."]},{"cell_type":"markdown","metadata":{},"source":["# Data Collection"]},{"cell_type":"markdown","metadata":{},"source":["The way we intend to obtain our training data is to get a few images of the students and perform one-shot learning with them, such that we do not have to waste the students' time with gathering large quantities of pictures for each of them."]},{"cell_type":"markdown","metadata":{},"source":["Firstly, we will be using opencv to make use of the webcam as the input device, since we are not building a physical prototype. We also use a haar cascade classifier in order to detect whether there are faces in the webcame image."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Cascade Classifier loaded successfully\n"]}],"source":["# Use the first video capturing device on your device\n","video=cv2.VideoCapture(0)\n","\n","# For this, copy the absolute path and replace it with your own\n","facedetect=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n","\n","# Check if cascade classifier loaded (whether path to cascade classifier is valid)\n","if facedetect.empty():\n","    print(\"Error: Cascade Classifier not loaded\")\n","else:\n","    print(\"Cascade Classifier loaded successfully\")"]},{"cell_type":"markdown","metadata":{},"source":["We then run this chunk where the user will be asked to input their name (not case-sensitive) and the webcam will start capturing images of the person if there is a face in the webcam. The images are then cropped such that only the face is visible in the image. 15 pictures total are taken of the student and saved to a folder which will be used for training and testing the model."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Name Already Taken\n"]},{"ename":"error","evalue":"OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)","Cell \u001b[1;32mIn[8], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \tcv2\u001b[38;5;241m.\u001b[39mimwrite(name, frame[y:y\u001b[38;5;241m+\u001b[39mh,x:x\u001b[38;5;241m+\u001b[39mw])\n\u001b[0;32m     27\u001b[0m \tcv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (x,y), (x\u001b[38;5;241m+\u001b[39mw, y\u001b[38;5;241m+\u001b[39mh), (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWindowFrame\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m test \u001b[38;5;129;01mand\u001b[39;00m count\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m10\u001b[39m:\n","\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n"]}],"source":["count=0\n","test = False\n","nameID=str(input(\"Enter Your Name: \")).lower()\n","\n","path='images/'+nameID\n","isExist = os.path.exists(path)\n","\n","# If name is already taken, ask for another one\n","if isExist:\n","\tprint(\"Name Already Taken\")\n","\tnameID=str(input(\"Enter Your Name Again: \"))\n","else:\n","\tos.makedirs(path)\n","\n","while True:\n","\t# Start reading input from webcam, shows webcam video capture on device\n","\tret,frame=video.read()\n","\tfaces=facedetect.detectMultiScale(frame,1.3, 5)\n","\tfor x,y,w,h in faces:\n","\t\tcount=count+1\n","\t\tif not test:\n","\t\t\tname='./images/train/'+nameID+'/'+ str(count) + '.jpg'\n","\t\telse:\n","\t\t\tname='./images/val/'+nameID+'/'+ str(count) + '.jpg'\n","\t\tprint(\"Creating Images.........\" +name)\n","\t\tcv2.imwrite(name, frame[y:y+h,x:x+w])\n","\t\tcv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,0), 3)\n","\tcv2.imshow(\"WindowFrame\", frame)\n","\tcv2.waitKey(1)\n","\tif not test and count>10:\n","\t\ttest = True\n","\t\tcontinue\n","\telif test and count>5:\n","\t\tbreak\n","\n","# Stops reading input from webcam, removes webcam video capture on device\n","video.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["Since our data is collected by hand, we do not need to check for missing values. Since our data collection method already makes use of the cascade classifier to ensure that a face is in frame and crops the image down to one of a fixed size and of the face only, we do not need to preprocess the data or check for any images that are not relevant.\n","\n","Since our aim is to perform one shot learning, we do not need to perform any data augmentation."]},{"cell_type":"markdown","metadata":{},"source":["# Creating a Model"]},{"cell_type":"markdown","metadata":{},"source":["For this project, we will create 2 models, one using transfer learning of MobileNet and the other being a model similar to FaceNEt, and compare them. Then based on the results, we will select the better one for our final product."]},{"cell_type":"markdown","metadata":{},"source":["First, we define the size of the images for our model."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["target_size = (224, 224)"]},{"cell_type":"markdown","metadata":{},"source":["Next, we load the datasets that we will be using for training."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 426 files belonging to 3 classes.\n"]},{"name":"stdout","output_type":"stream","text":["Found 427 files belonging to 3 classes.\n"]}],"source":["train = tf.keras.utils.image_dataset_from_directory('images/train', labels = 'inferred', image_size=target_size)\n","val = tf.keras.utils.image_dataset_from_directory('images/val', labels = 'inferred', image_size=target_size)"]},{"cell_type":"markdown","metadata":{},"source":["## Deep Learning Model: MobileNet Transfer Learning CNN "]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T10:56:54.431431Z","iopub.status.busy":"2024-02-28T10:56:54.431124Z","iopub.status.idle":"2024-02-28T10:56:54.454683Z","shell.execute_reply":"2024-02-28T10:56:54.453749Z","shell.execute_reply.started":"2024-02-28T10:56:54.431407Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\g0dph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n","\n","WARNING:tensorflow:From c:\\Users\\g0dph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n"]}],"source":["mobile_model = MobileNetV2(weights='imagenet', input_shape = target_size + (3,), include_top=False)\n","flat_layer = Flatten()(mobile_model.output)\n","dense_layer_1 = Dense(1024, activation='relu')(flat_layer)\n","normal_layer_1 = BatchNormalization()(dense_layer_1)\n","dense_layer_2 = Dense(1024, activation='relu')(normal_layer_1)\n","normal_layer_2 = BatchNormalization()(dense_layer_2)\n","dense_layer_3 = Dense(1024, activation='relu')(normal_layer_2)\n","normal_layer_3 = BatchNormalization()(dense_layer_3)\n","dense_layer_4 = Dense(256, activation='relu')(normal_layer_3)\n","normal_layer_4 = BatchNormalization()(dense_layer_4)\n","output_layer = Dense(3, activation='relu')(normal_layer_4)\n","\n","for layer in mobile_model.layers:\n","    layer.trainable = False\n","\n","transfer_mobile_model = Model(inputs = mobile_model.inputs, outputs = output_layer)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T10:56:54.456109Z","iopub.status.busy":"2024-02-28T10:56:54.455803Z","iopub.status.idle":"2024-02-28T11:07:18.862820Z","shell.execute_reply":"2024-02-28T11:07:18.861872Z","shell.execute_reply.started":"2024-02-28T10:56:54.456086Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","WARNING:tensorflow:From c:\\Users\\g0dph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n","\n","WARNING:tensorflow:From c:\\Users\\g0dph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n","\n","14/14 [==============================] - 6s 311ms/step - loss: 0.1359 - accuracy: 0.9437 - val_loss: 10.4723 - val_accuracy: 0.6464\n","Epoch 2/10\n","14/14 [==============================] - 4s 279ms/step - loss: 0.0329 - accuracy: 0.9930 - val_loss: 9.2059 - val_accuracy: 0.6300\n","Epoch 3/10\n","14/14 [==============================] - 4s 281ms/step - loss: 0.0300 - accuracy: 0.9906 - val_loss: 11.1579 - val_accuracy: 0.6230\n","Epoch 4/10\n","14/14 [==============================] - 4s 276ms/step - loss: 0.0501 - accuracy: 0.9930 - val_loss: 8.5597 - val_accuracy: 0.6393\n","Epoch 5/10\n","14/14 [==============================] - 4s 274ms/step - loss: 0.0082 - accuracy: 0.9977 - val_loss: 4.7214 - val_accuracy: 0.6464\n","Epoch 6/10\n","14/14 [==============================] - 4s 277ms/step - loss: 0.0311 - accuracy: 0.9883 - val_loss: 3.9302 - val_accuracy: 0.6862\n","Epoch 7/10\n","14/14 [==============================] - 4s 270ms/step - loss: 0.0172 - accuracy: 0.9977 - val_loss: 5.7700 - val_accuracy: 0.7026\n","Epoch 8/10\n","14/14 [==============================] - 4s 272ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 5.5214 - val_accuracy: 0.6838\n","Epoch 9/10\n","14/14 [==============================] - 4s 270ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 4.6466 - val_accuracy: 0.7026\n","Epoch 10/10\n","14/14 [==============================] - 4s 268ms/step - loss: 6.1778e-04 - accuracy: 1.0000 - val_loss: 4.4036 - val_accuracy: 0.6956\n"]}],"source":["transfer_mobile_model.compile(optimizer=tf.optimizers.Adam(),\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","transferhistory = transfer_mobile_model.fit(train, validation_data=val, epochs=10)"]},{"cell_type":"markdown","metadata":{},"source":["## Deep Learning Model: FaceNet Copy"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def inception_block_1a(X):\n","    X_3=Conv2D(96,(1,1),data_format='channels_first',name='inception_3a_3x3_conv1')(X)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3a_3x3_bn1')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    X_3=ZeroPadding2D(padding=(1,1),data_format='channels_first')(X_3)\n","    X_3=Conv2D(128,(3,3),data_format='channels_first',name='inception_3a_3x3_conv2')(X_3)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3a_3x3_bn2')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    \n","    X_5=Conv2D(16,(1,1),data_format='channels_first',name='inception_3a_5x5_conv1')(X)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3a_5x5_bn1')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    X_5=ZeroPadding2D(padding=(2,2),data_format='channels_first')(X_5)\n","    X_5=Conv2D(32,(5,5),data_format='channels_first',name='inception_3a_5x5_conv2')(X_5)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3a_5x5_bn2')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    \n","    X_pool = MaxPooling2D(pool_size=3, strides=2, data_format='channels_first')(X)\n","    X_pool = Conv2D(32, (1, 1), data_format='channels_first', name='inception_3a_pool_conv')(X_pool)\n","    X_pool = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_pool_bn')(X_pool)\n","    X_pool = Activation('relu')(X_pool)\n","    X_pool = ZeroPadding2D(padding=((3, 4), (3, 4)), data_format='channels_first')(X_pool)\n","    \n","    X_1=Conv2D(64,(1,1),data_format='channels_first',name='inception_3a_1x1_conv')(X)\n","    X_1=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3a_1x1_bn')(X_1)\n","    X_1=Activation('relu')(X_1)\n","    \n","    inception=concatenate([X_3,X_5,X_pool,X_1],axis=1)\n","    return inception\n","\n","def inception_block_1b(X):\n","    X_3=Conv2D(96,(1,1),data_format='channels_first',name='inception_3b_3x3_conv1')(X)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3b_3x3_bn1')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    X_3=ZeroPadding2D(padding=(1,1),data_format='channels_first')(X_3)\n","    X_3=Conv2D(128,(3,3),data_format='channels_first',name='inception_3b_3x3_conv2')(X_3)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3b_3x3_bn2')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    \n","    X_5=Conv2D(32,(1,1),data_format='channels_first',name='inception_3b_5x5_conv1')(X)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3b_5x5_bn1')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    X_5=ZeroPadding2D(padding=(2,2),data_format='channels_first')(X_5)\n","    X_5=Conv2D(64,(5,5),data_format='channels_first',name='inception_3b_5x5_conv2')(X_5)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3b_5x5_bn2')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    \n","    X_P=AveragePooling2D(pool_size=(3,3),strides=(3,3),data_format='channels_first')(X)\n","    X_P=Conv2D(64,(1,1),data_format='channels_first',name='inception_3b_pool_conv')(X_P)\n","    X_P=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3b_pool_bn')(X_P)\n","    X_P=Activation('relu')(X_P)\n","    X_P=ZeroPadding2D(padding=(4,4),data_format='channels_first')(X_P)\n","    \n","    X_1=Conv2D(64,(1,1),data_format='channels_first',name='inception_3b_1x1_conv')(X)\n","    X_1=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3b_1x1_bn')(X_1)\n","    X_1=Activation('relu')(X_1)\n","    \n","    inception=concatenate([X_3,X_5,X_P,X_1],axis=1)\n","    return inception\n","\n","def inception_block_1c(X):\n","    X_3=Conv2D(128,(1,1),data_format='channels_first',name='inception_3c_3x3_conv1')(X)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3c_3x3_bn1')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    X_3=ZeroPadding2D(padding=(1,1),data_format='channels_first')(X_3)\n","    X_3=Conv2D(256,(3,3),strides=(2,2),data_format='channels_first',name='inception_3c_3x3_conv2')(X_3)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3c_3x3_bn2')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    \n","    X_5=Conv2D(32,(1,1),data_format='channels_first',name='inception_3c_5x5_conv1')(X)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3c_5x5_bn1')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    X_5=ZeroPadding2D(padding=(2,2),data_format='channels_first')(X_5)\n","    X_5=Conv2D(64,(5,5),strides=(2,2),data_format='channels_first',name='inception_3c_5x5_conv2')(X_5)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3c_5x5_bn2')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    \n","    X_P=MaxPooling2D(pool_size=3,strides=2,data_format='channels_first')(X)\n","    X_P=ZeroPadding2D(padding=((0,1),(0,1)),data_format='channels_first')(X_P)\n","    \n","\n","    inception=concatenate([X_3,X_5,X_P],axis=1)\n","    return inception\n","\n","def inception_block_2a(X):\n","    X_3=Conv2D(96,(1,1),data_format='channels_first',name='inception_4a_3x3_conv1')(X)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4a_3x3_bn1')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    X_3=ZeroPadding2D(padding=(1,1),data_format='channels_first')(X_3)\n","    X_3=Conv2D(192,(3,3),data_format='channels_first',name='inception_4a_3x3_conv2')(X_3)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4a_3x3_bn2')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    \n","    X_5=Conv2D(32,(1,1),data_format='channels_first',name='inception_4a_5x5_conv1')(X)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4a_5x5_bn1')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    X_5=ZeroPadding2D(padding=(2,2),data_format='channels_first')(X_5)\n","    X_5=Conv2D(64,(5,5),data_format='channels_first',name='inception_4a_5x5_conv2')(X_5)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4a_5x5_bn2')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    \n","    X_P=AveragePooling2D(pool_size=(3,3),strides=(3,3),data_format='channels_first')(X)\n","    X_P=Conv2D(128,(1,1),data_format='channels_first',name='inception_4a_pool_conv')(X_P)\n","    X_P=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4a_pool_bn')(X_P)\n","    X_P=Activation('relu')(X_P)\n","    X_P=ZeroPadding2D(padding=(2,2),data_format='channels_first')(X_P)\n","    \n","    X_1=Conv2D(256,(1,1),data_format='channels_first',name='inception_4a_1x1_conv')(X)\n","    X_1=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4a_1x1_bn')(X_1)\n","    X_1=Activation('relu')(X_1)\n","    \n","    inception=concatenate([X_3,X_5,X_P,X_1],axis=1)\n","    return inception\n","\n","def inception_block_2b(X):\n","    X_3=Conv2D(160,(1,1),data_format='channels_first',name='inception_4e_3x3_conv1')(X)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4e_3x3_bn1')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    X_3=ZeroPadding2D(padding=(1,1),data_format='channels_first')(X_3)\n","    X_3=Conv2D(256,(3,3),strides=(2,2),data_format='channels_first',name='inception_4e_3x3_conv2')(X_3)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4e_3x3_bn2')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    \n","    X_5=Conv2D(64,(1,1),data_format='channels_first',name='inception_4e_5x5_conv1')(X)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4e_5x5_bn1')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    X_5=ZeroPadding2D(padding=(2,2),data_format='channels_first')(X_5)\n","    X_5=Conv2D(128,(5,5),strides=(2,2),data_format='channels_first',name='inception_4e_5x5_conv2')(X_5)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4e_5x5_bn2')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    \n","    X_P=MaxPooling2D(pool_size=3,strides=2,data_format='channels_first')(X)\n","    X_P=ZeroPadding2D(padding=((0,1),(0,1)),data_format='channels_first')(X_P)\n","\n","    inception=concatenate([X_3,X_5,X_P],axis=1)\n","    return inception\n","\n","def inception_block_3a(X):\n","    X_3=Conv2D(96,(1,1),data_format='channels_first',name='inception_5a_3x3_conv1')(X)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_5a_3x3_bn1')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    X_3=ZeroPadding2D(padding=(1,1),data_format='channels_first')(X_3)\n","    X_3=Conv2D(384,(3,3),data_format='channels_first',name='inception_5a_3x3_conv2')(X_3)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_5a_3x3_bn2')(X_3)\n","    X_3=Activation('relu')(X_3)\n","\n","    \n","    X_P=AveragePooling2D(pool_size=(3,3),strides=(3,3),data_format='channels_first')(X)\n","    X_P=Conv2D(96,(1,1),data_format='channels_first',name='inception_5a_pool_conv')(X_P)\n","    X_P=BatchNormalization(axis=1,epsilon=0.00001,name='inception_5a_pool_bn')(X_P)\n","    X_P=Activation('relu')(X_P)\n","    X_P=ZeroPadding2D(padding=(1,1),data_format='channels_first')(X_P)\n","    \n","    X_1=Conv2D(256,(1,1),data_format='channels_first',name='inception_5a_1x1_conv')(X)\n","    X_1=BatchNormalization(axis=1,epsilon=0.00001,name='inception_5a_1x1_bn')(X_1)\n","    X_1=Activation('relu')(X_1)\n","    \n","    inception=concatenate([X_3,X_P,X_1],axis=1)\n","    return inception\n","def inception_block_3b(X):\n","    X_3=Conv2D(96,(1,1),data_format='channels_first',name='inception_5b_3x3_conv1')(X)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_5b_3x3_bn1')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    X_3=ZeroPadding2D(padding=(1,1),data_format='channels_first')(X_3)\n","    X_3=Conv2D(384,(3,3),data_format='channels_first',name='inception_5b_3x3_conv2')(X_3)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_5b_3x3_bn2')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    \n","    \n","    X_P=MaxPooling2D(pool_size=(3,3),strides=2,data_format='channels_first')(X)\n","    X_P=Conv2D(96,(1,1),data_format='channels_first',name='inception_5b_pool_conv')(X_P)\n","    X_P=BatchNormalization(axis=1,epsilon=0.00001,name='inception_5b_pool_bn')(X_P)\n","    X_P=Activation('relu')(X_P)\n","    X_P=ZeroPadding2D(padding=(1,1),data_format='channels_first')(X_P)\n","    \n","    X_1=Conv2D(256,(1,1),data_format='channels_first',name='inception_5b_1x1_conv')(X)\n","    X_1=BatchNormalization(axis=1,epsilon=0.00001,name='inception_5b_1x1_bn')(X_1)\n","    X_1=Activation('relu')(X_1)\n","    \n","    inception=concatenate([X_3,X_P,X_1],axis=1)\n","    return inception\n","\n","\n","def FinalModel(input_shape):\n","    \n","    X_input=Input(input_shape)\n","    \n","    X=ZeroPadding2D(padding=(3,3))(X_input)\n","    X=Conv2D(64,(7,7),strides=(2,2),name='conv1')(X)\n","    X=BatchNormalization(axis=1,name='bn1')(X)\n","    X=Activation('relu')(X)\n","    \n","    X=ZeroPadding2D((1,1))(X)\n","    X=MaxPooling2D((3,3),strides=2)(X)\n","    \n","    X=Conv2D(64,(1,1),strides=(1,1),name='conv2')(X)\n","    X=BatchNormalization(axis=1,epsilon=0.00001,name='bn2')(X)\n","    X=Activation('relu')(X) \n","    \n","    X=ZeroPadding2D(padding=(1,1))(X)\n","    \n","    X=Conv2D(192,(3,3),strides=(1,1),name='conv3')(X)\n","    X=BatchNormalization(axis=1,epsilon=0.00001,name='bn3')(X)\n","    X=Activation('relu')(X)\n","    \n","    X=ZeroPadding2D(padding=(1,1))(X)\n","    X=MaxPooling2D(pool_size=(3,3),strides=(2,2))(X)\n","    \n","    X=inception_block_1a(X)\n","    X=inception_block_1b(X)\n","    X=inception_block_1c(X)\n","    \n","    X=inception_block_2a(X)\n","    X=inception_block_2b(X)\n","    \n","    X=inception_block_3a(X)\n","    X=inception_block_3b(X)\n","    \n","    X=AveragePooling2D(pool_size=(3,3),strides=(1,1),data_format='channels_first')(X)\n","    X=Flatten()(X)\n","    X=Dense(128,activation='relu',kernel_initializer='glorot_normal',name='dense_layer')(X)\n","    X=Lambda(lambda x:K.l2_normalize(x,axis=1),name='lambda_1')(X)\n","    \n","    model=Model(inputs=X_input,outputs=X,name='FaceRecognotionModel')\n","    return model    "]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation of Models"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["labels = next(os.walk(\"images/train\"))[1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["count = 0\n","while True:\n","\tsuccess, imgOrignal=cap.read()\n","\tfaces = facedetect.detectMultiScale(imgOrignal,1.3,5)\n","\tfor x,y,w,h in faces:\n","\t\tcrop_img=imgOrignal[y:y+h,x:x+h]\n","\t\timg=cv2.resize(crop_img, (224,224))\n","\t\timg=img.reshape(1, 224, 224, 3)\n","\t\tprediction=model.predict(img)\n","\t\tclassIndex = np.argmax(prediction)\n","\n","\t\tcv2.rectangle(imgOrignal,(x,y),(x+w,y+h),(0,255,0),2)\n","\t\tcv2.rectangle(imgOrignal, (x,y-40),(x+w, y), (0,255,0),-2)\n","\t\tcv2.putText(imgOrignal, labels[classIndex],(x,y-10), cv2.FONT_HERSHEY_COMPLEX, 0.75, (255,255,255),1, cv2.LINE_AA)\n","\t\tcount += 1\n","\t\tif count > 20:\n","\t\t\tcount = 0\n","\t\t\tprint('index 0')\n","\t\t\timport sys\n","\t\t\tsys.exit()\n","\t\t\t\n","\n","\tcv2.imshow(\"Result\",imgOrignal)\n","\tk=cv2.waitKey(1)\n","\tif k==ord('q'):\n","\t\tbreak\n","\n","\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{},"source":["# Hyperparameter Tuning"]},{"cell_type":"markdown","metadata":{},"source":["Now, we will tune our model. First, we remake the model that we chose, that being the V3model so that we can tune it."]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T11:07:47.287079Z","iopub.status.busy":"2024-02-28T11:07:47.286725Z","iopub.status.idle":"2024-02-28T11:07:49.180786Z","shell.execute_reply":"2024-02-28T11:07:49.180020Z","shell.execute_reply.started":"2024-02-28T11:07:47.287046Z"},"trusted":true},"outputs":[],"source":["inceptionthemovie = tf.keras.applications.inception_v3.InceptionV3(\n","    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n","    include_top=False\n",")\n","inceptionthemovie.trainable = False\n","input = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n","temp = data_augmentation(input)\n","temp = tf.keras.applications.inception_v3.preprocess_input(temp)\n","temp = inceptionthemovie(temp, training=False)\n","temp = tf.keras.layers.GlobalAveragePooling2D()(temp)\n","\n","finalmodel = tf.keras.Model(input, tf.keras.layers.Dense(10)(temp))"]},{"cell_type":"markdown","metadata":{},"source":["Then we will use the keras tuner in order to find the best hyperparameters for our model. In this case, only the learning rate is applicable."]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T11:07:49.182100Z","iopub.status.busy":"2024-02-28T11:07:49.181834Z","iopub.status.idle":"2024-02-28T11:07:49.222727Z","shell.execute_reply":"2024-02-28T11:07:49.221753Z","shell.execute_reply.started":"2024-02-28T11:07:49.182077Z"},"trusted":true},"outputs":[],"source":["import keras_tuner as kt\n","def model_builder(hp):\n","    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n","    # hp_dropout = hp.Float('dropout', min_value=0.1, max_value=0.8, step=0.1) #unused\n","    # hp_dense_units = hp.Int('units', min_value=128, max_value=1024, step=64) #unused\n","\n","    model = finalmodel\n","\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n","                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                metrics=['accuracy'])\n","    \n","    return model\n","\n","\n","tuner = kt.Hyperband(model_builder,\n","                     objective='val_accuracy',\n","                     max_epochs=30,\n","                     factor=3,\n","                     directory='dir')"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T11:07:49.224121Z","iopub.status.busy":"2024-02-28T11:07:49.223800Z","iopub.status.idle":"2024-02-28T11:14:19.783101Z","shell.execute_reply":"2024-02-28T11:14:19.782128Z","shell.execute_reply.started":"2024-02-28T11:07:49.224096Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 3 Complete [00h 02m 09s]\n","val_accuracy: 0.8529999852180481\n","\n","Best val_accuracy So Far: 0.8585000038146973\n","Total elapsed time: 00h 06m 31s\n"]}],"source":["tuner.search(train, epochs=30, validation_data=val)\n","best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"]},{"cell_type":"markdown","metadata":{},"source":["Finally, we attempt to fit the model to find the epoch with the best accuracy before we do a final evaluation of our model."]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T11:14:19.784412Z","iopub.status.busy":"2024-02-28T11:14:19.784112Z","iopub.status.idle":"2024-02-28T11:44:05.394484Z","shell.execute_reply":"2024-02-28T11:44:05.393592Z","shell.execute_reply.started":"2024-02-28T11:14:19.784385Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 590ms/step - accuracy: 0.8345 - loss: 0.5455 - val_accuracy: 0.8660 - val_loss: 0.4584\n","Epoch 2/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 569ms/step - accuracy: 0.8300 - loss: 0.5451 - val_accuracy: 0.8685 - val_loss: 0.4545\n","Epoch 3/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 563ms/step - accuracy: 0.8315 - loss: 0.5270 - val_accuracy: 0.8695 - val_loss: 0.4530\n","Epoch 4/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8352 - loss: 0.5213 - val_accuracy: 0.8705 - val_loss: 0.4512\n","Epoch 5/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 560ms/step - accuracy: 0.8350 - loss: 0.5142 - val_accuracy: 0.8700 - val_loss: 0.4503\n","Epoch 6/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 562ms/step - accuracy: 0.8313 - loss: 0.5212 - val_accuracy: 0.8720 - val_loss: 0.4491\n","Epoch 7/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8308 - loss: 0.5099 - val_accuracy: 0.8695 - val_loss: 0.4486\n","Epoch 8/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8298 - loss: 0.5130 - val_accuracy: 0.8715 - val_loss: 0.4476\n","Epoch 9/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 566ms/step - accuracy: 0.8387 - loss: 0.5110 - val_accuracy: 0.8740 - val_loss: 0.4459\n","Epoch 10/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 563ms/step - accuracy: 0.8435 - loss: 0.4806 - val_accuracy: 0.8730 - val_loss: 0.4472\n","Epoch 11/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8298 - loss: 0.5171 - val_accuracy: 0.8735 - val_loss: 0.4442\n","Epoch 12/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 566ms/step - accuracy: 0.8385 - loss: 0.5086 - val_accuracy: 0.8720 - val_loss: 0.4449\n","Epoch 13/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8358 - loss: 0.5007 - val_accuracy: 0.8760 - val_loss: 0.4441\n","Epoch 14/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8354 - loss: 0.4921 - val_accuracy: 0.8740 - val_loss: 0.4443\n","Epoch 15/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 568ms/step - accuracy: 0.8407 - loss: 0.4903 - val_accuracy: 0.8715 - val_loss: 0.4445\n","Epoch 16/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 567ms/step - accuracy: 0.8431 - loss: 0.4803 - val_accuracy: 0.8740 - val_loss: 0.4422\n","Epoch 17/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 571ms/step - accuracy: 0.8404 - loss: 0.4927 - val_accuracy: 0.8720 - val_loss: 0.4424\n","Epoch 18/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8449 - loss: 0.4785 - val_accuracy: 0.8765 - val_loss: 0.4423\n","Epoch 19/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8427 - loss: 0.4846 - val_accuracy: 0.8745 - val_loss: 0.4415\n","Epoch 20/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 569ms/step - accuracy: 0.8442 - loss: 0.4748 - val_accuracy: 0.8740 - val_loss: 0.4411\n","Epoch 21/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8453 - loss: 0.4865 - val_accuracy: 0.8750 - val_loss: 0.4405\n","Epoch 22/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 568ms/step - accuracy: 0.8420 - loss: 0.4809 - val_accuracy: 0.8750 - val_loss: 0.4403\n","Epoch 23/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 562ms/step - accuracy: 0.8389 - loss: 0.4867 - val_accuracy: 0.8750 - val_loss: 0.4401\n","Epoch 24/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 561ms/step - accuracy: 0.8431 - loss: 0.4869 - val_accuracy: 0.8740 - val_loss: 0.4412\n","Epoch 25/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8443 - loss: 0.4739 - val_accuracy: 0.8710 - val_loss: 0.4405\n","Epoch 26/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8389 - loss: 0.4886 - val_accuracy: 0.8755 - val_loss: 0.4395\n","Epoch 27/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 566ms/step - accuracy: 0.8404 - loss: 0.4871 - val_accuracy: 0.8750 - val_loss: 0.4395\n","Epoch 28/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 569ms/step - accuracy: 0.8440 - loss: 0.4838 - val_accuracy: 0.8760 - val_loss: 0.4396\n","Epoch 29/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 566ms/step - accuracy: 0.8447 - loss: 0.4703 - val_accuracy: 0.8735 - val_loss: 0.4397\n","Epoch 30/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 560ms/step - accuracy: 0.8421 - loss: 0.4817 - val_accuracy: 0.8720 - val_loss: 0.4396\n"]}],"source":["tuningmodel = tuner.hypermodel.build(best_hps)\n","history = tuningmodel.fit(train, epochs=30, validation_data=val, callbacks=[callbacks])\n","\n","val_acc_per_epoch = history.history['val_accuracy']\n","best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T11:44:05.395933Z","iopub.status.busy":"2024-02-28T11:44:05.395648Z","iopub.status.idle":"2024-02-28T12:02:37.892276Z","shell.execute_reply":"2024-02-28T12:02:37.891307Z","shell.execute_reply.started":"2024-02-28T11:44:05.395907Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 595ms/step - accuracy: 0.8415 - loss: 0.4734 - val_accuracy: 0.8720 - val_loss: 0.4397\n","Epoch 2/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 567ms/step - accuracy: 0.8336 - loss: 0.4774 - val_accuracy: 0.8740 - val_loss: 0.4382\n","Epoch 3/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8455 - loss: 0.4807 - val_accuracy: 0.8760 - val_loss: 0.4379\n","Epoch 4/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 565ms/step - accuracy: 0.8456 - loss: 0.4779 - val_accuracy: 0.8740 - val_loss: 0.4389\n","Epoch 5/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 568ms/step - accuracy: 0.8446 - loss: 0.4756 - val_accuracy: 0.8725 - val_loss: 0.4386\n","Epoch 6/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 567ms/step - accuracy: 0.8478 - loss: 0.4612 - val_accuracy: 0.8750 - val_loss: 0.4384\n","Epoch 7/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 566ms/step - accuracy: 0.8451 - loss: 0.4666 - val_accuracy: 0.8730 - val_loss: 0.4387\n","Epoch 8/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 567ms/step - accuracy: 0.8467 - loss: 0.4738 - val_accuracy: 0.8720 - val_loss: 0.4390\n","Epoch 9/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8516 - loss: 0.4596 - val_accuracy: 0.8735 - val_loss: 0.4372\n","Epoch 10/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8416 - loss: 0.4805 - val_accuracy: 0.8755 - val_loss: 0.4377\n","Epoch 11/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 565ms/step - accuracy: 0.8431 - loss: 0.4714 - val_accuracy: 0.8760 - val_loss: 0.4370\n","Epoch 12/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8516 - loss: 0.4611 - val_accuracy: 0.8725 - val_loss: 0.4371\n","Epoch 13/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 563ms/step - accuracy: 0.8469 - loss: 0.4770 - val_accuracy: 0.8735 - val_loss: 0.4363\n","Epoch 14/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 568ms/step - accuracy: 0.8436 - loss: 0.4727 - val_accuracy: 0.8725 - val_loss: 0.4370\n","Epoch 15/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 566ms/step - accuracy: 0.8499 - loss: 0.4763 - val_accuracy: 0.8745 - val_loss: 0.4363\n","Epoch 16/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8497 - loss: 0.4613 - val_accuracy: 0.8735 - val_loss: 0.4365\n","Epoch 17/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 568ms/step - accuracy: 0.8501 - loss: 0.4600 - val_accuracy: 0.8740 - val_loss: 0.4375\n","Epoch 18/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 567ms/step - accuracy: 0.8512 - loss: 0.4645 - val_accuracy: 0.8755 - val_loss: 0.4362\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 433ms/step - accuracy: 0.8854 - loss: 0.4152\n"]},{"data":{"text/plain":["[0.4361916482448578, 0.8755000233650208]"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["finalmodel = tuner.hypermodel.build(best_hps)\n","\n","finalmodel.fit(train, epochs=best_epoch, validation_data=val)\n","finalmodel.evaluate(val)"]},{"cell_type":"markdown","metadata":{},"source":["# Final Model Evaluation\n","```py\n","before = [0.4680415689945221, 0.8709999918937683]\n","after = [0.4361916482448578, 0.8755000233650208]\n","```\n","From this, we can conclude that finetuning has increased the accuracy slightly by around 0.55% and decreased the loss by around 0.03, which means the hyperparameter tuning has worked and improved the model. Overall, the model is quite accurate and categorising pictures into the 10 different classes."]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3087285,"sourceId":5311975,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
