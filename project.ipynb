{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Installation of Libraries Required"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pip install opencv-python numpy tensorflow"]},{"cell_type":"markdown","metadata":{},"source":["## Importing Libraries & Defining the Image Capturing Device"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T10:48:48.026279Z","iopub.status.busy":"2024-02-28T10:48:48.025369Z","iopub.status.idle":"2024-02-28T10:48:48.030785Z","shell.execute_reply":"2024-02-28T10:48:48.029849Z","shell.execute_reply.started":"2024-02-28T10:48:48.026247Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.15.0\n"]}],"source":["import os\n","import cv2\n","import random\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import *\n","from keras.layers import *\n","from keras import backend as K\n","from keras.models import Model\n","from tensorflow.keras.applications import mobilenet_v2\n","from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n","K.set_image_data_format('channels_first')\n","\n","print(tf.__version__)"]},{"cell_type":"markdown","metadata":{},"source":["Setting the video capturing device and making sure that an input device is detected"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"text/plain":["False"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["cap=cv2.VideoCapture(0)\n","cap.set(3, 640)\n","cap.set(4, 480)"]},{"cell_type":"markdown","metadata":{},"source":["# Problem Statement"]},{"cell_type":"markdown","metadata":{},"source":["#### This project intends to provide an alternative to the fingerprint scanners at our boarding school using facial recognition. The project only intends to cover the backend, and there will be no physical prototype. Hardware used for demonstration will be web camera from computer."]},{"cell_type":"markdown","metadata":{},"source":["# Data Collection"]},{"cell_type":"markdown","metadata":{},"source":["The way we intend to obtain our training data is to get a few images of the students and perform one-shot learning with them, such that we do not have to waste the students' time with gathering large quantities of pictures for each of them."]},{"cell_type":"markdown","metadata":{},"source":["Firstly, we will be using opencv to make use of the webcam as the input device, since we are not building a physical prototype. We also use a haar cascade classifier in order to detect whether there are faces in the webcame image."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Cascade Classifier loaded successfully\n"]}],"source":["# Use the first video capturing device on your device\n","video=cv2.VideoCapture(0)\n","\n","# For this, copy the absolute path and replace it with your own\n","# This is the casecade classifier we will be using to detect whether a person is in any given image\n","facedetect=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n","\n","# Check if cascade classifier loaded (whether path to cascade classifier is valid)\n","if facedetect.empty():\n","    print(\"Error: Cascade Classifier not loaded\")\n","else:\n","    print(\"Cascade Classifier loaded successfully\")"]},{"cell_type":"markdown","metadata":{},"source":["We then run this chunk where the user will be asked to input their name (not case-sensitive) and the webcam will start capturing images of the person if there is a face in the webcam. The images are then cropped such that only the face is visible in the image. 8 pictures total are taken of the student and saved to a folder which will be used for training and testing the model. 5 of which are for training (one of them script.jpg which will serve as the anchor image of each person when creating triplets for the second model using the triplet loss function), and 3 of them are for validation, which is used only for the first model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["count=0\n","test = False\n","anchor = True\n","nameID=str(input(\"Enter Your Name: \")).lower()\n","\n","path='images/'+nameID\n","isExist = os.path.exists(path)\n","\n","# If name is already taken, ask for another one\n","if isExist:\n","\tprint(\"Name Already Taken\")\n","\tnameID=str(input(\"Enter Your Name Again: \"))\n","else:\n","\tos.makedirs(path)\n","\n","while True:\n","\t# Start reading input from webcam, shows webcam video capture on device\n","\tret,frame=video.read()\n","\tfaces=facedetect.detectMultiScale(frame,1.3, 5)\n","\tfor x,y,w,h in faces:\n","\t\tcount=count+1\n","\t\tif not test and anchor:\n","\t\t\tname='./images/train/'+nameID+'/script.jpg'\n","\t\t\tanchor = False\n","\t\telif not test and not anchor:\n","\t\t\tname='./images/train/'+nameID+'/'+ str(count) + '.jpg'\n","\t\telse:\n","\t\t\tname='./images/val/'+nameID+'/'+ str(count) + '.jpg'\n","\t\tprint(\"Creating Images...\" +name)\n","\t\tcv2.imwrite(name, frame[y:y+h,x:x+w])\n","\t\tcv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,0), 3)\n","\tcv2.imshow(\"WindowFrame\", frame)\n","\tcv2.waitKey(1)\n","\tif not test and count>5:\n","\t\ttest = True\n","\t\tcount = 0\n","\t\tcontinue\n","\telif test and count>3:\n","\t\tbreak\n","\n","# Stops reading input from webcam, removes webcam video capture on device\n","video.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["Since our data is collected by hand, we do not need to check for missing values. Since our data collection method already makes use of the cascade classifier to ensure that a face is in frame and crops the image down to one of a fixed size and of the face only, we do not need to preprocess the data or check for any images that are not relevant.\n","\n","Since our aim is to perform one shot learning, we do not need to perform any data augmentation."]},{"cell_type":"markdown","metadata":{},"source":["# Creating a Model"]},{"cell_type":"markdown","metadata":{},"source":["For this project, we will create 2 models, one using transfer learning of MobileNet and the other being a model similar to FaceNEt, and compare them. Then based on the results, we will select the better one for our final product."]},{"cell_type":"markdown","metadata":{},"source":["First, we define the size of the images for our model."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["target_size = (224, 224)"]},{"cell_type":"markdown","metadata":{},"source":["## Deep Learning Model: MobileNet Transfer Learning CNN "]},{"cell_type":"markdown","metadata":{},"source":["For this model, we will be using transfer learning on MobileNetV2, since this is what is the model supposedly used by teachablemachine (website that trains models for you based on data input, includes face recognition). However, since only the weights are given and the actual model summary is not given, we can only attempt to make our simple CNN model to emulate what is in teachable machine. Since we will be using a normal CNN so we will be splitting the training data using the normal method."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train = tf.keras.utils.image_dataset_from_directory('images/train', labels = 'inferred', image_size=target_size)\n","val = tf.keras.utils.image_dataset_from_directory('images/val', labels = 'inferred', image_size=target_size)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T10:56:54.431431Z","iopub.status.busy":"2024-02-28T10:56:54.431124Z","iopub.status.idle":"2024-02-28T10:56:54.454683Z","shell.execute_reply":"2024-02-28T10:56:54.453749Z","shell.execute_reply.started":"2024-02-28T10:56:54.431407Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\g0dph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n","\n","WARNING:tensorflow:From c:\\Users\\g0dph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n"]}],"source":["mobile_model = MobileNetV2(weights='imagenet', input_shape = target_size + (3,), include_top=False)\n","flat_layer = Flatten()(mobile_model.output)\n","dense_layer_1 = Dense(1024, activation='relu')(flat_layer)\n","normal_layer_1 = BatchNormalization()(dense_layer_1)\n","dense_layer_2 = Dense(1024, activation='relu')(normal_layer_1)\n","normal_layer_2 = BatchNormalization()(dense_layer_2)\n","dense_layer_3 = Dense(1024, activation='relu')(normal_layer_2)\n","normal_layer_3 = BatchNormalization()(dense_layer_3)\n","dense_layer_4 = Dense(256, activation='relu')(normal_layer_3)\n","normal_layer_4 = BatchNormalization()(dense_layer_4)\n","output_layer = Dense(3, activation='relu')(normal_layer_4)\n","\n","for layer in mobile_model.layers:\n","    layer.trainable = False\n","\n","transfer_mobile_model = Model(inputs = mobile_model.inputs, outputs = output_layer)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T10:56:54.456109Z","iopub.status.busy":"2024-02-28T10:56:54.455803Z","iopub.status.idle":"2024-02-28T11:07:18.862820Z","shell.execute_reply":"2024-02-28T11:07:18.861872Z","shell.execute_reply.started":"2024-02-28T10:56:54.456086Z"},"trusted":true},"outputs":[],"source":["transfer_mobile_model.compile(optimizer=tf.optimizers.Adam(),\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","transferhistory = transfer_mobile_model.fit(train, validation_data=val, epochs=5)"]},{"cell_type":"markdown","metadata":{},"source":["## Deep Learning Model: Advanced CNN Model utilising Triplet Loss Function "]},{"cell_type":"markdown","metadata":{},"source":["For the second model, we will be taking the model given in https://www.kaggle.com/code/amankumarmallik/one-shot-learning-for-face-verification, which aims to create a model which is able to perform one shot learning for image recognition. It uses a triplet loss function which is where it differs from other models, taking in 3 images at a time. These 3 are the anchor, positive and negative image. The positive image is an image of the person in the anchor image, except in a different context. The negative image would then be an image of another person that is not the one in the anchor image. "]},{"cell_type":"markdown","metadata":{},"source":["First, we want to create the triplets for the triplet loss function. The data_gen() function essentially pulls 2 random pictures of 1 person and another picture of a different person, reformats them using the localize_resize() function, and then outputs the triplet arrays."]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["def localize_resize(path_image,facedetect):\n","    image=cv2.imread(path_image)\n","    \n","    # Checks if the number of faces etected in the photo is more than 1, it will not be used. However, due to the way we created the data, this should not be a problem and only serves as a contingency\n","    gray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n","    classifier= facedetect\n","    faces=classifier.detectMultiScale(gray,1.1,6)\n","    if len(faces) != 1: \n","        return []\n","    \n","    # Crops images down to just the face\n","    x,y,w,h=faces.squeeze()\n","    crop=image[y:y+h,x:x+w]\n","\n","    # Other reformatting \n","    image=cv2.resize(crop,(96,96))\n","    image=np.transpose(image,(2,0,1))\n","    image=image.astype('float32')/255.0 # makes pixel values between 0 and 1\n","    return image\n","\n","def data_gen(batch_size=32):\n","    while True:\n","        i=0\n","        positive=[]\n","        anchor=[]\n","        negative=[]    \n","        \n","        while(i<batch_size):\n","            id=os.listdir('images/train')\n","            ra=random.sample(id,2)\n","            pos_dir='images/train'+'/'+ra[0]\n","            neg_dir='images/train'+'/'+ra[1]\n","            pos=pos_dir+'/'+random.choice(os.listdir(pos_dir))\n","            anc=pos_dir+'/'+random.choice([x for x in os.listdir(pos_dir) if 'script' in x])\n","            neg=neg_dir+'/'+random.choice(os.listdir(neg_dir))\n","\n","            # Resize all of the images to a standard format and ensure that each pixel value ranges between 0 and 1\n","            pos_img=localize_resize(pos,facedetect)\n","            if len(pos_img) == 0:\n","                continue\n","            neg_img=localize_resize(neg,facedetect)\n","            if len(neg_img) == 0:\n","                continue\n","            anc_img=localize_resize(anc,facedetect)\n","            if len(anc_img) == 0:\n","                continue\n","            \n","            positive.append(list(pos_img))\n","            negative.append(list(neg_img))\n","            anchor.append(list(anc_img))\n","            i=i+1\n","        yield (np.array([np.array(anchor), np.array(positive), np.array(negative)]), np.zeros((batch_size,1)).astype(\"float32\"))"]},{"cell_type":"markdown","metadata":{},"source":["Next, we have the base model which will be what actually classifies our images."]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["def inception_block_1a(X):\n","    X_3=Conv2D(96,(1,1),data_format='channels_first',name='inception_3a_3x3_conv1')(X)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3a_3x3_bn1')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    X_3=ZeroPadding2D(padding=(1,1),data_format='channels_first')(X_3)\n","    X_3=Conv2D(128,(3,3),data_format='channels_first',name='inception_3a_3x3_conv2')(X_3)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3a_3x3_bn2')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    \n","    X_5=Conv2D(16,(1,1),data_format='channels_first',name='inception_3a_5x5_conv1')(X)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3a_5x5_bn1')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    X_5=ZeroPadding2D(padding=(2,2),data_format='channels_first')(X_5)\n","    X_5=Conv2D(32,(5,5),data_format='channels_first',name='inception_3a_5x5_conv2')(X_5)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3a_5x5_bn2')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    \n","    X_pool = MaxPooling2D(pool_size=3, strides=2, data_format='channels_first')(X)\n","    X_pool = Conv2D(32, (1, 1), data_format='channels_first', name='inception_3a_pool_conv')(X_pool)\n","    X_pool = BatchNormalization(axis=1, epsilon=0.00001, name='inception_3a_pool_bn')(X_pool)\n","    X_pool = Activation('relu')(X_pool)\n","    X_pool = ZeroPadding2D(padding=((3, 4), (3, 4)), data_format='channels_first')(X_pool)\n","    \n","    X_1=Conv2D(64,(1,1),data_format='channels_first',name='inception_3a_1x1_conv')(X)\n","    X_1=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3a_1x1_bn')(X_1)\n","    X_1=Activation('relu')(X_1)\n","    \n","    inception=concatenate([X_3,X_5,X_pool,X_1],axis=1)\n","    return inception\n","\n","def inception_block_1b(X):\n","    X_3=Conv2D(96,(1,1),data_format='channels_first',name='inception_3b_3x3_conv1')(X)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3b_3x3_bn1')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    X_3=ZeroPadding2D(padding=(1,1),data_format='channels_first')(X_3)\n","    X_3=Conv2D(128,(3,3),data_format='channels_first',name='inception_3b_3x3_conv2')(X_3)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3b_3x3_bn2')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    \n","    X_5=Conv2D(32,(1,1),data_format='channels_first',name='inception_3b_5x5_conv1')(X)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3b_5x5_bn1')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    X_5=ZeroPadding2D(padding=(2,2),data_format='channels_first')(X_5)\n","    X_5=Conv2D(64,(5,5),data_format='channels_first',name='inception_3b_5x5_conv2')(X_5)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3b_5x5_bn2')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    \n","    X_P=AveragePooling2D(pool_size=(3,3),strides=(3,3),data_format='channels_first')(X)\n","    X_P=Conv2D(64,(1,1),data_format='channels_first',name='inception_3b_pool_conv')(X_P)\n","    X_P=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3b_pool_bn')(X_P)\n","    X_P=Activation('relu')(X_P)\n","    X_P=ZeroPadding2D(padding=(4,4),data_format='channels_first')(X_P)\n","    \n","    X_1=Conv2D(64,(1,1),data_format='channels_first',name='inception_3b_1x1_conv')(X)\n","    X_1=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3b_1x1_bn')(X_1)\n","    X_1=Activation('relu')(X_1)\n","    \n","    inception=concatenate([X_3,X_5,X_P,X_1],axis=1)\n","    return inception\n","\n","def inception_block_1c(X):\n","    X_3=Conv2D(128,(1,1),data_format='channels_first',name='inception_3c_3x3_conv1')(X)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3c_3x3_bn1')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    X_3=ZeroPadding2D(padding=(1,1),data_format='channels_first')(X_3)\n","    X_3=Conv2D(256,(3,3),strides=(2,2),data_format='channels_first',name='inception_3c_3x3_conv2')(X_3)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3c_3x3_bn2')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    \n","    X_5=Conv2D(32,(1,1),data_format='channels_first',name='inception_3c_5x5_conv1')(X)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3c_5x5_bn1')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    X_5=ZeroPadding2D(padding=(2,2),data_format='channels_first')(X_5)\n","    X_5=Conv2D(64,(5,5),strides=(2,2),data_format='channels_first',name='inception_3c_5x5_conv2')(X_5)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_3c_5x5_bn2')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    \n","    X_P=MaxPooling2D(pool_size=3,strides=2,data_format='channels_first')(X)\n","    X_P=ZeroPadding2D(padding=((0,1),(0,1)),data_format='channels_first')(X_P)\n","    \n","\n","    inception=concatenate([X_3,X_5,X_P],axis=1)\n","    return inception\n","\n","def inception_block_2a(X):\n","    X_3=Conv2D(96,(1,1),data_format='channels_first',name='inception_4a_3x3_conv1')(X)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4a_3x3_bn1')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    X_3=ZeroPadding2D(padding=(1,1),data_format='channels_first')(X_3)\n","    X_3=Conv2D(192,(3,3),data_format='channels_first',name='inception_4a_3x3_conv2')(X_3)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4a_3x3_bn2')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    \n","    X_5=Conv2D(32,(1,1),data_format='channels_first',name='inception_4a_5x5_conv1')(X)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4a_5x5_bn1')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    X_5=ZeroPadding2D(padding=(2,2),data_format='channels_first')(X_5)\n","    X_5=Conv2D(64,(5,5),data_format='channels_first',name='inception_4a_5x5_conv2')(X_5)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4a_5x5_bn2')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    \n","    X_P=AveragePooling2D(pool_size=(3,3),strides=(3,3),data_format='channels_first')(X)\n","    X_P=Conv2D(128,(1,1),data_format='channels_first',name='inception_4a_pool_conv')(X_P)\n","    X_P=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4a_pool_bn')(X_P)\n","    X_P=Activation('relu')(X_P)\n","    X_P=ZeroPadding2D(padding=(2,2),data_format='channels_first')(X_P)\n","    \n","    X_1=Conv2D(256,(1,1),data_format='channels_first',name='inception_4a_1x1_conv')(X)\n","    X_1=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4a_1x1_bn')(X_1)\n","    X_1=Activation('relu')(X_1)\n","    \n","    inception=concatenate([X_3,X_5,X_P,X_1],axis=1)\n","    return inception\n","\n","def inception_block_2b(X):\n","    X_3=Conv2D(160,(1,1),data_format='channels_first',name='inception_4e_3x3_conv1')(X)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4e_3x3_bn1')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    X_3=ZeroPadding2D(padding=(1,1),data_format='channels_first')(X_3)\n","    X_3=Conv2D(256,(3,3),strides=(2,2),data_format='channels_first',name='inception_4e_3x3_conv2')(X_3)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4e_3x3_bn2')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    \n","    X_5=Conv2D(64,(1,1),data_format='channels_first',name='inception_4e_5x5_conv1')(X)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4e_5x5_bn1')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    X_5=ZeroPadding2D(padding=(2,2),data_format='channels_first')(X_5)\n","    X_5=Conv2D(128,(5,5),strides=(2,2),data_format='channels_first',name='inception_4e_5x5_conv2')(X_5)\n","    X_5=BatchNormalization(axis=1,epsilon=0.00001,name='inception_4e_5x5_bn2')(X_5)\n","    X_5=Activation('relu')(X_5)\n","    \n","    X_P=MaxPooling2D(pool_size=3,strides=2,data_format='channels_first')(X)\n","    X_P=ZeroPadding2D(padding=((0,1),(0,1)),data_format='channels_first')(X_P)\n","\n","    inception=concatenate([X_3,X_5,X_P],axis=1)\n","    return inception\n","\n","def inception_block_3a(X):\n","    X_3=Conv2D(96,(1,1),data_format='channels_first',name='inception_5a_3x3_conv1')(X)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_5a_3x3_bn1')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    X_3=ZeroPadding2D(padding=(1,1),data_format='channels_first')(X_3)\n","    X_3=Conv2D(384,(3,3),data_format='channels_first',name='inception_5a_3x3_conv2')(X_3)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_5a_3x3_bn2')(X_3)\n","    X_3=Activation('relu')(X_3)\n","\n","    \n","    X_P=AveragePooling2D(pool_size=(3,3),strides=(3,3),data_format='channels_first')(X)\n","    X_P=Conv2D(96,(1,1),data_format='channels_first',name='inception_5a_pool_conv')(X_P)\n","    X_P=BatchNormalization(axis=1,epsilon=0.00001,name='inception_5a_pool_bn')(X_P)\n","    X_P=Activation('relu')(X_P)\n","    X_P=ZeroPadding2D(padding=(1,1),data_format='channels_first')(X_P)\n","    \n","    X_1=Conv2D(256,(1,1),data_format='channels_first',name='inception_5a_1x1_conv')(X)\n","    X_1=BatchNormalization(axis=1,epsilon=0.00001,name='inception_5a_1x1_bn')(X_1)\n","    X_1=Activation('relu')(X_1)\n","    \n","    inception=concatenate([X_3,X_P,X_1],axis=1)\n","    return inception\n","def inception_block_3b(X):\n","    X_3=Conv2D(96,(1,1),data_format='channels_first',name='inception_5b_3x3_conv1')(X)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_5b_3x3_bn1')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    X_3=ZeroPadding2D(padding=(1,1),data_format='channels_first')(X_3)\n","    X_3=Conv2D(384,(3,3),data_format='channels_first',name='inception_5b_3x3_conv2')(X_3)\n","    X_3=BatchNormalization(axis=1,epsilon=0.00001,name='inception_5b_3x3_bn2')(X_3)\n","    X_3=Activation('relu')(X_3)\n","    \n","    \n","    X_P=MaxPooling2D(pool_size=(3,3),strides=2,data_format='channels_first')(X)\n","    X_P=Conv2D(96,(1,1),data_format='channels_first',name='inception_5b_pool_conv')(X_P)\n","    X_P=BatchNormalization(axis=1,epsilon=0.00001,name='inception_5b_pool_bn')(X_P)\n","    X_P=Activation('relu')(X_P)\n","    X_P=ZeroPadding2D(padding=(1,1),data_format='channels_first')(X_P)\n","    \n","    X_1=Conv2D(256,(1,1),data_format='channels_first',name='inception_5b_1x1_conv')(X)\n","    X_1=BatchNormalization(axis=1,epsilon=0.00001,name='inception_5b_1x1_bn')(X_1)\n","    X_1=Activation('relu')(X_1)\n","    \n","    inception=concatenate([X_3,X_P,X_1],axis=1)\n","    return inception\n","\n","\n","def FinalModel(input_shape):\n","    \n","    X_input=Input(input_shape)\n","    \n","    X=ZeroPadding2D(padding=(3,3))(X_input)\n","    X=Conv2D(64,(7,7),strides=(2,2),name='conv1')(X)\n","    X=BatchNormalization(axis=1,name='bn1')(X)\n","    X=Activation('relu')(X)\n","    \n","    X=ZeroPadding2D((1,1))(X)\n","    X=MaxPooling2D((3,3),strides=2)(X)\n","    \n","    X=Conv2D(64,(1,1),strides=(1,1),name='conv2')(X)\n","    X=BatchNormalization(axis=1,epsilon=0.00001,name='bn2')(X)\n","    X=Activation('relu')(X) \n","    \n","    X=ZeroPadding2D(padding=(1,1))(X)\n","    \n","    X=Conv2D(192,(3,3),strides=(1,1),name='conv3')(X)\n","    X=BatchNormalization(axis=1,epsilon=0.00001,name='bn3')(X)\n","    X=Activation('relu')(X)\n","    \n","    X=ZeroPadding2D(padding=(1,1))(X)\n","    X=MaxPooling2D(pool_size=(3,3),strides=(2,2))(X)\n","    \n","    X=inception_block_1a(X)\n","    X=inception_block_1b(X)\n","    X=inception_block_1c(X)\n","    \n","    X=inception_block_2a(X)\n","    X=inception_block_2b(X)\n","    \n","    X=inception_block_3a(X)\n","    X=inception_block_3b(X)\n","    \n","    X=AveragePooling2D(pool_size=(3,3),strides=(1,1),data_format='channels_first')(X)\n","    X=Flatten()(X)\n","    X=Dense(128,activation='relu',kernel_initializer='glorot_normal',name='dense_layer')(X)\n","    X=Lambda(lambda x:K.l2_normalize(x,axis=1),name='lambda_1')(X)\n","    \n","    model=Model(inputs=X_input,outputs=X,name='FaceRecognotionModel')\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["We then wrap the base model with 3 input nodes and 1 output node, such that it can take in 3 images at a time. We also define the custom triplet loss function we will be using here."]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_6\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_26 (InputLayer)       [(None, 3, 96, 96)]          0         []                            \n","                                                                                                  \n"," input_28 (InputLayer)       [(None, 3, 96, 96)]          0         []                            \n","                                                                                                  \n"," input_27 (InputLayer)       [(None, 3, 96, 96)]          0         []                            \n","                                                                                                  \n"," FaceRecognotionModel (Func  (None, 128)                  3743280   ['input_26[0][0]',            \n"," tional)                                                             'input_28[0][0]',            \n","                                                                     'input_27[0][0]']            \n","                                                                                                  \n"," concatenate_55 (Concatenat  (None, 384)                  0         ['FaceRecognotionModel[0][0]',\n"," e)                                                                  'FaceRecognotionModel[1][0]',\n","                                                                     'FaceRecognotionModel[2][0]']\n","                                                                                                  \n","==================================================================================================\n","Total params: 3743280 (14.28 MB)\n","Trainable params: 3733968 (14.24 MB)\n","Non-trainable params: 9312 (36.38 KB)\n","__________________________________________________________________________________________________\n"]}],"source":["model=FinalModel(input_shape=(3,96,96))\n","\n","def triplet_loss_t(y_true,y_pred):\n","    anchor=y_pred[:,0:128]\n","    pos=y_pred[:,128:256]\n","    neg=y_pred[:,256:384]\n","    \n","    positive_distance = K.sum(K.abs(anchor-pos), axis=1)\n","    negative_distance = K.sum(K.abs(anchor-neg), axis=1)\n","    probs=K.softmax([positive_distance,negative_distance],axis=0)\n","    loss=K.mean(K.abs(probs[0])+K.abs(1.0-probs[1]))\n","    return loss\n","\n","triplet_model_a=Input((3,96,96))\n","triplet_model_n=Input((3,96,96))\n","triplet_model_p=Input((3,96,96))\n","triplet_model_out=Concatenate()([model(triplet_model_a),model(triplet_model_p),model(triplet_model_n)])\n","triplet_model=Model([triplet_model_a,triplet_model_p,triplet_model_n],triplet_model_out)\n","triplet_model.compile(optimizer='adam',loss=triplet_loss_t)\n","triplet_model.summary()"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["next(data_gen())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n"]},{"ename":"AttributeError","evalue":"in user code:\n\n    File \"c:\\Users\\g0dph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\g0dph\\AppData\\Local\\Temp\\ipykernel_23400\\3881667300.py\", line 10, in triplet_loss_t  *\n        probs=K.softmax([positive_distance,negative_distance],axis=0)\n    File \"c:\\Users\\g0dph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py\", line 5442, in softmax\n        if x.shape.rank <= 1:\n\n    AttributeError: 'list' object has no attribute 'shape'\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtriplet_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\g0dph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileava_pgdw.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filec0ce437c.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__triplet_loss_t\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     13\u001b[0m positive_distance \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(K)\u001b[38;5;241m.\u001b[39msum, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(K)\u001b[38;5;241m.\u001b[39mabs, (ag__\u001b[38;5;241m.\u001b[39mld(anchor) \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(pos),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n\u001b[0;32m     14\u001b[0m negative_distance \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(K)\u001b[38;5;241m.\u001b[39msum, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(K)\u001b[38;5;241m.\u001b[39mabs, (ag__\u001b[38;5;241m.\u001b[39mld(anchor) \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(neg),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n\u001b[1;32m---> 15\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositive_distance\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnegative_distance\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(K)\u001b[38;5;241m.\u001b[39mmean, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(K)\u001b[38;5;241m.\u001b[39mabs, (ag__\u001b[38;5;241m.\u001b[39mld(probs)[\u001b[38;5;241m0\u001b[39m],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(K)\u001b[38;5;241m.\u001b[39mabs, (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(probs)[\u001b[38;5;241m1\u001b[39m],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"c:\\Users\\g0dph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\g0dph\\AppData\\Local\\Temp\\ipykernel_23400\\3881667300.py\", line 10, in triplet_loss_t  *\n        probs=K.softmax([positive_distance,negative_distance],axis=0)\n    File \"c:\\Users\\g0dph\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py\", line 5442, in softmax\n        if x.shape.rank <= 1:\n\n    AttributeError: 'list' object has no attribute 'shape'\n"]}],"source":["triplet_model.fit(data_gen(), steps_per_epoch=100,epochs=5)"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation of Models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["labels = next(os.walk(\"images/train\"))[1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["count = 0\n","while True:\n","\tsuccess, imgOrignal=cap.read()\n","\tfaces = facedetect.detectMultiScale(imgOrignal,1.3,5)\n","\tfor x,y,w,h in faces:\n","\t\tcrop_img=imgOrignal[y:y+h,x:x+h]\n","\t\timg=cv2.resize(crop_img, (224,224))\n","\t\timg=img.reshape(1, 224, 224, 3)\n","\t\tprediction=model.predict(img)\n","\t\tclassIndex = np.argmax(prediction)\n","\n","\t\tcv2.rectangle(imgOrignal,(x,y),(x+w,y+h),(0,255,0),2)\n","\t\tcv2.rectangle(imgOrignal, (x,y-40),(x+w, y), (0,255,0),-2)\n","\t\tcv2.putText(imgOrignal, labels[classIndex],(x,y-10), cv2.FONT_HERSHEY_COMPLEX, 0.75, (255,255,255),1, cv2.LINE_AA)\n","\t\tcount += 1\n","\t\tif count > 20:\n","\t\t\tcount = 0\n","\t\t\tprint('index 0')\n","\t\t\timport sys\n","\t\t\tsys.exit()\n","\t\t\t\n","\n","\tcv2.imshow(\"Result\",imgOrignal)\n","\tk=cv2.waitKey(1)\n","\tif k==ord('q'):\n","\t\tbreak\n","\n","\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{},"source":["# Hyperparameter Tuning"]},{"cell_type":"markdown","metadata":{},"source":["Now, we will tune our model. First, we remake the model that we chose, that being the V3model so that we can tune it."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T11:07:47.287079Z","iopub.status.busy":"2024-02-28T11:07:47.286725Z","iopub.status.idle":"2024-02-28T11:07:49.180786Z","shell.execute_reply":"2024-02-28T11:07:49.180020Z","shell.execute_reply.started":"2024-02-28T11:07:47.287046Z"},"trusted":true},"outputs":[],"source":["inceptionthemovie = tf.keras.applications.inception_v3.InceptionV3(\n","    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n","    include_top=False\n",")\n","inceptionthemovie.trainable = False\n","input = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n","temp = data_augmentation(input)\n","temp = tf.keras.applications.inception_v3.preprocess_input(temp)\n","temp = inceptionthemovie(temp, training=False)\n","temp = tf.keras.layers.GlobalAveragePooling2D()(temp)\n","\n","finalmodel = tf.keras.Model(input, tf.keras.layers.Dense(10)(temp))"]},{"cell_type":"markdown","metadata":{},"source":["Then we will use the keras tuner in order to find the best hyperparameters for our model. In this case, only the learning rate is applicable."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T11:07:49.182100Z","iopub.status.busy":"2024-02-28T11:07:49.181834Z","iopub.status.idle":"2024-02-28T11:07:49.222727Z","shell.execute_reply":"2024-02-28T11:07:49.221753Z","shell.execute_reply.started":"2024-02-28T11:07:49.182077Z"},"trusted":true},"outputs":[],"source":["import keras_tuner as kt\n","def model_builder(hp):\n","    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n","    # hp_dropout = hp.Float('dropout', min_value=0.1, max_value=0.8, step=0.1) #unused\n","    # hp_dense_units = hp.Int('units', min_value=128, max_value=1024, step=64) #unused\n","\n","    model = finalmodel\n","\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n","                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                metrics=['accuracy'])\n","    \n","    return model\n","\n","\n","tuner = kt.Hyperband(model_builder,\n","                     objective='val_accuracy',\n","                     max_epochs=30,\n","                     factor=3,\n","                     directory='dir')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T11:07:49.224121Z","iopub.status.busy":"2024-02-28T11:07:49.223800Z","iopub.status.idle":"2024-02-28T11:14:19.783101Z","shell.execute_reply":"2024-02-28T11:14:19.782128Z","shell.execute_reply.started":"2024-02-28T11:07:49.224096Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 3 Complete [00h 02m 09s]\n","val_accuracy: 0.8529999852180481\n","\n","Best val_accuracy So Far: 0.8585000038146973\n","Total elapsed time: 00h 06m 31s\n"]}],"source":["tuner.search(train, epochs=30, validation_data=val)\n","best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"]},{"cell_type":"markdown","metadata":{},"source":["Finally, we attempt to fit the model to find the epoch with the best accuracy before we do a final evaluation of our model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T11:14:19.784412Z","iopub.status.busy":"2024-02-28T11:14:19.784112Z","iopub.status.idle":"2024-02-28T11:44:05.394484Z","shell.execute_reply":"2024-02-28T11:44:05.393592Z","shell.execute_reply.started":"2024-02-28T11:14:19.784385Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 590ms/step - accuracy: 0.8345 - loss: 0.5455 - val_accuracy: 0.8660 - val_loss: 0.4584\n","Epoch 2/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 569ms/step - accuracy: 0.8300 - loss: 0.5451 - val_accuracy: 0.8685 - val_loss: 0.4545\n","Epoch 3/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 563ms/step - accuracy: 0.8315 - loss: 0.5270 - val_accuracy: 0.8695 - val_loss: 0.4530\n","Epoch 4/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8352 - loss: 0.5213 - val_accuracy: 0.8705 - val_loss: 0.4512\n","Epoch 5/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 560ms/step - accuracy: 0.8350 - loss: 0.5142 - val_accuracy: 0.8700 - val_loss: 0.4503\n","Epoch 6/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 562ms/step - accuracy: 0.8313 - loss: 0.5212 - val_accuracy: 0.8720 - val_loss: 0.4491\n","Epoch 7/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8308 - loss: 0.5099 - val_accuracy: 0.8695 - val_loss: 0.4486\n","Epoch 8/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8298 - loss: 0.5130 - val_accuracy: 0.8715 - val_loss: 0.4476\n","Epoch 9/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 566ms/step - accuracy: 0.8387 - loss: 0.5110 - val_accuracy: 0.8740 - val_loss: 0.4459\n","Epoch 10/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 563ms/step - accuracy: 0.8435 - loss: 0.4806 - val_accuracy: 0.8730 - val_loss: 0.4472\n","Epoch 11/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8298 - loss: 0.5171 - val_accuracy: 0.8735 - val_loss: 0.4442\n","Epoch 12/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 566ms/step - accuracy: 0.8385 - loss: 0.5086 - val_accuracy: 0.8720 - val_loss: 0.4449\n","Epoch 13/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8358 - loss: 0.5007 - val_accuracy: 0.8760 - val_loss: 0.4441\n","Epoch 14/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8354 - loss: 0.4921 - val_accuracy: 0.8740 - val_loss: 0.4443\n","Epoch 15/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 568ms/step - accuracy: 0.8407 - loss: 0.4903 - val_accuracy: 0.8715 - val_loss: 0.4445\n","Epoch 16/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 567ms/step - accuracy: 0.8431 - loss: 0.4803 - val_accuracy: 0.8740 - val_loss: 0.4422\n","Epoch 17/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 571ms/step - accuracy: 0.8404 - loss: 0.4927 - val_accuracy: 0.8720 - val_loss: 0.4424\n","Epoch 18/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8449 - loss: 0.4785 - val_accuracy: 0.8765 - val_loss: 0.4423\n","Epoch 19/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8427 - loss: 0.4846 - val_accuracy: 0.8745 - val_loss: 0.4415\n","Epoch 20/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 569ms/step - accuracy: 0.8442 - loss: 0.4748 - val_accuracy: 0.8740 - val_loss: 0.4411\n","Epoch 21/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8453 - loss: 0.4865 - val_accuracy: 0.8750 - val_loss: 0.4405\n","Epoch 22/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 568ms/step - accuracy: 0.8420 - loss: 0.4809 - val_accuracy: 0.8750 - val_loss: 0.4403\n","Epoch 23/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 562ms/step - accuracy: 0.8389 - loss: 0.4867 - val_accuracy: 0.8750 - val_loss: 0.4401\n","Epoch 24/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 561ms/step - accuracy: 0.8431 - loss: 0.4869 - val_accuracy: 0.8740 - val_loss: 0.4412\n","Epoch 25/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8443 - loss: 0.4739 - val_accuracy: 0.8710 - val_loss: 0.4405\n","Epoch 26/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8389 - loss: 0.4886 - val_accuracy: 0.8755 - val_loss: 0.4395\n","Epoch 27/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 566ms/step - accuracy: 0.8404 - loss: 0.4871 - val_accuracy: 0.8750 - val_loss: 0.4395\n","Epoch 28/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 569ms/step - accuracy: 0.8440 - loss: 0.4838 - val_accuracy: 0.8760 - val_loss: 0.4396\n","Epoch 29/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 566ms/step - accuracy: 0.8447 - loss: 0.4703 - val_accuracy: 0.8735 - val_loss: 0.4397\n","Epoch 30/30\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 560ms/step - accuracy: 0.8421 - loss: 0.4817 - val_accuracy: 0.8720 - val_loss: 0.4396\n"]}],"source":["tuningmodel = tuner.hypermodel.build(best_hps)\n","history = tuningmodel.fit(train, epochs=30, validation_data=val, callbacks=[callbacks])\n","\n","val_acc_per_epoch = history.history['val_accuracy']\n","best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-28T11:44:05.395933Z","iopub.status.busy":"2024-02-28T11:44:05.395648Z","iopub.status.idle":"2024-02-28T12:02:37.892276Z","shell.execute_reply":"2024-02-28T12:02:37.891307Z","shell.execute_reply.started":"2024-02-28T11:44:05.395907Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 595ms/step - accuracy: 0.8415 - loss: 0.4734 - val_accuracy: 0.8720 - val_loss: 0.4397\n","Epoch 2/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 567ms/step - accuracy: 0.8336 - loss: 0.4774 - val_accuracy: 0.8740 - val_loss: 0.4382\n","Epoch 3/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8455 - loss: 0.4807 - val_accuracy: 0.8760 - val_loss: 0.4379\n","Epoch 4/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 565ms/step - accuracy: 0.8456 - loss: 0.4779 - val_accuracy: 0.8740 - val_loss: 0.4389\n","Epoch 5/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 568ms/step - accuracy: 0.8446 - loss: 0.4756 - val_accuracy: 0.8725 - val_loss: 0.4386\n","Epoch 6/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 567ms/step - accuracy: 0.8478 - loss: 0.4612 - val_accuracy: 0.8750 - val_loss: 0.4384\n","Epoch 7/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 566ms/step - accuracy: 0.8451 - loss: 0.4666 - val_accuracy: 0.8730 - val_loss: 0.4387\n","Epoch 8/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 567ms/step - accuracy: 0.8467 - loss: 0.4738 - val_accuracy: 0.8720 - val_loss: 0.4390\n","Epoch 9/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8516 - loss: 0.4596 - val_accuracy: 0.8735 - val_loss: 0.4372\n","Epoch 10/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8416 - loss: 0.4805 - val_accuracy: 0.8755 - val_loss: 0.4377\n","Epoch 11/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 565ms/step - accuracy: 0.8431 - loss: 0.4714 - val_accuracy: 0.8760 - val_loss: 0.4370\n","Epoch 12/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 565ms/step - accuracy: 0.8516 - loss: 0.4611 - val_accuracy: 0.8725 - val_loss: 0.4371\n","Epoch 13/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 563ms/step - accuracy: 0.8469 - loss: 0.4770 - val_accuracy: 0.8735 - val_loss: 0.4363\n","Epoch 14/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 568ms/step - accuracy: 0.8436 - loss: 0.4727 - val_accuracy: 0.8725 - val_loss: 0.4370\n","Epoch 15/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 566ms/step - accuracy: 0.8499 - loss: 0.4763 - val_accuracy: 0.8745 - val_loss: 0.4363\n","Epoch 16/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 564ms/step - accuracy: 0.8497 - loss: 0.4613 - val_accuracy: 0.8735 - val_loss: 0.4365\n","Epoch 17/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 568ms/step - accuracy: 0.8501 - loss: 0.4600 - val_accuracy: 0.8740 - val_loss: 0.4375\n","Epoch 18/18\n","\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 567ms/step - accuracy: 0.8512 - loss: 0.4645 - val_accuracy: 0.8755 - val_loss: 0.4362\n","\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 433ms/step - accuracy: 0.8854 - loss: 0.4152\n"]},{"data":{"text/plain":["[0.4361916482448578, 0.8755000233650208]"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["finalmodel = tuner.hypermodel.build(best_hps)\n","\n","finalmodel.fit(train, epochs=best_epoch, validation_data=val)\n","finalmodel.evaluate(val)"]},{"cell_type":"markdown","metadata":{},"source":["# Final Model Evaluation\n","```py\n","before = [0.4680415689945221, 0.8709999918937683]\n","after = [0.4361916482448578, 0.8755000233650208]\n","```\n","From this, we can conclude that finetuning has increased the accuracy slightly by around 0.55% and decreased the loss by around 0.03, which means the hyperparameter tuning has worked and improved the model. Overall, the model is quite accurate and categorising pictures into the 10 different classes."]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3087285,"sourceId":5311975,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
